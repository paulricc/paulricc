{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10280232,"sourceType":"datasetVersion","datasetId":6361466}],"dockerImageVersionId":30822,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.arima.model import ARIMA\nimport warnings\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/stoxx-2014-2024/STOXX - 2014-2024.csv')\ndata = data.drop(columns='Close')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Identifying and Handling outliers","metadata":{}},{"cell_type":"code","source":"def handling_outliers(df):\n    outliers_info = {}\n    for column in df.select_dtypes(include = ['number']).columns.to_list():\n        mean = df[column].mean()\n        std = df[column].std()\n        outliers = df[(df[column] < mean-3*std) | (df[column] > mean+3*std)]\n        outliers_info[column] = len(outliers)\n        median = df[column].median()\n        df.loc[(df[column] < mean-3*std) | (df[column] > mean+3*std), column] = median\n    return outliers_info, df\n\n\noutliers_info, data = handling_outliers(data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"outliers_info","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Handling missing values","metadata":{}},{"cell_type":"code","source":"missing_values = data.isnull().sum().sum()\nmissing_values","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.interpolate(method='linear', inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.isnull().sum().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ARIMA ","metadata":{}},{"cell_type":"markdown","source":"## Initial Analysis","metadata":{}},{"cell_type":"code","source":"data['Date'] = pd.to_datetime(data['Date'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(data['Date'], data['Adj Close'])\nplt.xticks(rotation=45)\nplt.xlabel('Time')\nplt.ylabel('Adjusted Close price')\nplt.title('STOXX Europe 600 Adjusted close price (2014 - 2024)')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_acf(data['Adj Close'], lags=40, alpha=0.05)\nplt.xlabel('Lags')\nplt.ylabel('Autocorrelation')\nplt.title('Autocorrelation Function')\nplt.ylim(-0.1, 1.1)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_pacf(data['Adj Close'], lags=40, alpha=0.05)\nplt.xlabel('Lags')\nplt.ylabel('Autocorrelation')\nplt.title('Partial Autocorrelation Function')\nplt.ylim(-0.1, 1.1)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ad_full_result = adfuller(data['Adj Close'])\nadf_results = pd.DataFrame({\n    'Metric': [\n        'ADF Statistic',\n        'p-value',\n        'Number of lags used',\n        'Number of observations used'\n    ],\n    'Value': [\n        round(ad_full_result[0], 3),\n        round(ad_full_result[1], 3),\n        ad_full_result[2],\n        ad_full_result[3]\n    ]\n})\n\nadf_results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## First Differencing","metadata":{}},{"cell_type":"code","source":"data.set_index('Date', inplace=True)\nstoxx_diff = data['Adj Close'].diff()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(stoxx_diff)\nplt.xlabel('Time')\nplt.ylabel('STOXX 600')\nplt.title('First-differenced STOXX Europe 600')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stoxx_diff = stoxx_diff.dropna()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ad_full_result_diff = adfuller(stoxx_diff)\nadf_results_diff = pd.DataFrame({\n    'Metric': [\n        'ADF Statistic',\n        'p-value',\n        'Number of lags used',\n        'Number of observations used'\n    ],\n    'Value': [\n        round(ad_full_result_diff[0], 3),\n        round(ad_full_result_diff[1], 3),\n        ad_full_result_diff[2],\n        ad_full_result_diff[3]\n    ]\n})\n\nadf_results_diff","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_acf(stoxx_diff, lags=40, alpha=0.05)\nplt.xlabel('Lags')\nplt.ylabel('Autocorrelation')\nplt.title('Autocorrelation Function')\nplt.ylim(-0.1, 1.1)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_pacf(stoxx_diff, lags=40, alpha=0.05)\nplt.xlabel('Lags')\nplt.ylabel('Autocorrelation')\nplt.title('Autocorrelation Function')\nplt.ylim(-0.1, 1.1)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model selection","metadata":{}},{"cell_type":"code","source":"warnings.filterwarnings('ignore')\n\np_range = range(0, 6)\nd_range = [0, 1]\nq_range = range (0, 6)\n\nseries = data['Adj Close']\n\ntrain_size = int(len(series)*0.8)\ntrain_series = series[:train_size]\ntest_series = series[train_size:]\n\nresult = []\n\nfor p in p_range:\n    for d in d_range:\n        for q in q_range:\n            try:\n                model = ARIMA(train_series, order=(p,d,q))\n                res = model.fit()\n    \n                BIC = res.bic\n    \n                result.append({\n                    'p': p,\n                    'd': d,\n                    'q': q,\n                    'BIC': BIC\n                })\n            except Exception as e:\n                print(e)\n                continue\n\nresult_df = pd.DataFrame(result)\nsorted_results = result_df.sort_values(by='BIC')\n\nprint(sorted_results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"warnings.filterwarnings('ignore')\n\np_range = range(0, 6)\nd_range = [0, 1]\nq_range = range (0, 6)\n\nseries = data['Adj Close']\n\ntrain_size = int(len(series)*0.8)\ntrain_series = series[:train_size]\ntest_series = series[train_size:]\n\nresult = []\n\nfor p in p_range:\n    for d in d_range:\n        for q in q_range:\n            try:\n                model = ARIMA(train_series, order=(p,d,q))\n                res = model.fit()\n    \n                AIC = res.aic\n    \n                result.append({\n                    'p': p,\n                    'd': d,\n                    'q': q,\n                    'AIC': AIC\n                })\n            except Exception as e:\n                print(e)\n                continue\n\nresult_df = pd.DataFrame(result)\nsorted_results = result_df.sort_values(by='AIC')\n\nprint(sorted_results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Residuals check","metadata":{}},{"cell_type":"code","source":"# Best configurations for ARIMA: (p,d,q) --> (0,1,0) & (5,1,4)\n\nmodel = ARIMA(data['Adj Close'], order=(0,1,0))\narima_010_results = model.fit()\narima_010_residuals = arima_010_results.resid\n\nplt.plot(arima_010_residuals)\nplt.xlabel('Time')\nplt.ylabel('Residuals')\nplt.title('Residuals - ARIMA(0,1,0)')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = ARIMA(data['Adj Close'], order=(5,1,4))\narima_514_results = model.fit()\narima_514_residuals = arima_514_results.resid\n\nplt.plot(arima_514_residuals)\nplt.xlabel('Time')\nplt.ylabel('Residuals')\nplt.title('Residuals - ARIMA(5,1,4)')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prediction and model evaluation","metadata":{}},{"cell_type":"code","source":"def mean_absolute_percentage_error(y_pred, y_true):\n    y_pred, y_true = np.array(y_pred), np.array(y_true)\n    nonzero_indices = y_true != 0\n    if not np.any(nonzero_indices):\n        return np.inf\n    return np.mean(np.abs((y_pred[nonzero_indices] - y_true[nonzero_indices]) / y_true[nonzero_indices] )) * 100\n\n\ntrain_size = int(len(data)*0.8)\ntrain_sample, test_sample = data['Adj Close'][:train_size], data['Adj Close'][train_size:]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def rolling_forecast_1_day_ahead(train, test, order=(0,0,0)):\n    history = list(train)\n    predictions = []\n\n    for t in range(len(test)):\n        model = ARIMA(history, order=order)\n        model_fit = model.fit()\n        output = model_fit.forecast(steps=1)\n        pred = output[-1]\n    \n        predictions.append(pred)\n    \n        history.append(test.iloc[t])\n\n    return predictions\n\npredictions_1_day_ahead_010 = rolling_forecast_1_day_ahead(train_sample, test_sample, (0,1,0))\npredictions_1_day_ahead_111 = rolling_forecast_1_day_ahead(train_sample, test_sample, (1,1,1))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mape_010_1_day = mean_absolute_percentage_error(predictions_1_day_ahead_010, test_sample)\nrmse_010_1_day = mean_squared_error(test_sample, predictions_1_day_ahead_010)\nmae_010_1_day = mean_absolute_error(test_sample, predictions_1_day_ahead_010)\nr2_010_1_day = r2_score(test_sample, predictions_1_day_ahead_010)\n\nmape_111_1_day = mean_absolute_percentage_error(predictions_1_day_ahead_111, test_sample)\nrmse_111_1_day = mean_squared_error(test_sample, predictions_1_day_ahead_111)\nmae_111_1_day = mean_absolute_error(test_sample, predictions_1_day_ahead_111)\nr2_111_1_day = r2_score(test_sample, predictions_1_day_ahead_111)\n\nprint('Evaluation Metrics (1 day ahead) - ARIMA(0, 1, 0)\\n')\nprint(f\"RMSE: {rmse_010_1_day}\")\nprint(f\"MAPE: {mape_010_1_day}\")\nprint(f\"MAE: {mae_010_1_day}\")\nprint(f\"R^2: {r2_010_1_day}\")\n\nprint('\\n\\n')\nprint('Evaluation Metrics (1 day ahead) - ARIMA(1, 1, 1)\\n')\nprint(f\"RMSE: {rmse_111_1_day}\")\nprint(f\"MAPE: {mape_111_1_day}\")\nprint(f\"MAE: {mae_111_1_day}\")\nprint(f\"R^2: {r2_111_1_day}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def rolling_forecast_7_days_ahead(train, test, order=(0,0,0)):\n    history = list(train)\n    predictions = []\n\n    for t in range(len(test) - 6):\n        model = ARIMA(history, order=order)\n        model_fit = model.fit()\n        output = model_fit.forecast(steps=7)\n        pred = output[-1]\n    \n        predictions.append(pred)\n    \n        history.append(test.iloc[t])\n\n    return predictions\n\npredictions_7_days_ahead_010 = rolling_forecast_7_days_ahead(train_sample, test_sample, (0,1,0))\npredictions_7_days_ahead_111 = rolling_forecast_7_days_ahead(train_sample, test_sample, (1,1,1))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mape_010_7_days = mean_absolute_percentage_error(predictions_7_days_ahead_010, test_sample[6:])\nrmse_010_7_days = np.sqrt(mean_squared_error(test_sample[6:], predictions_7_days_ahead_010))\nmae_010_7_days = mean_absolute_error(test_sample[6:], predictions_7_days_ahead_010)\nr2_010_7_days = r2_score(test_sample[6:], predictions_7_days_ahead_010)\n\nmape_111_7_days = mean_absolute_percentage_error(predictions_7_days_ahead_111, test_sample[6:])\nrmse_111_7_days = np.sqrt(mean_squared_error(test_sample[6:], predictions_7_days_ahead_111))\nmae_111_7_days = mean_absolute_error(test_sample[6:], predictions_7_days_ahead_111)\nr2_111_7_days = r2_score(test_sample[6:], predictions_7_days_ahead_111)\n\nprint('Evaluation Metrics (7 days ahead) - ARIMA(0, 1, 0)\\n')\nprint(f\"RMSE: {rmse_010_7_days}\")\nprint(f\"MAPE: {mape_010_7_days}\")\nprint(f\"MAE: {mae_010_7_days}\")\nprint(f\"R^2: {r2_010_7_days}\")\n\nprint('\\n\\n')\nprint('Evaluation Metrics (7 days ahead) - ARIMA(1, 1, 1)\\n')\nprint(f\"RMSE: {rmse_111_7_days}\")\nprint(f\"MAPE: {mape_111_7_days}\")\nprint(f\"MAE: {mae_111_7_days}\")\nprint(f\"R^2: {r2_111_7_days}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def rolling_forecast_30_days_ahead(train, test, order=(0,0,0)):\n    history = list(train)\n    predictions = []\n\n    for t in range(len(test) - 29):\n        model = ARIMA(history, order=order)\n        model_fit = model.fit()\n        output = model_fit.forecast(steps=30)\n        pred = output[-1]\n    \n        predictions.append(pred)\n    \n        history.append(test.iloc[t])\n\n    return predictions\n\npredictions_30_days_ahead_010 = rolling_forecast_30_days_ahead(train_sample, test_sample, (0,1,0))\npredictions_30_days_ahead_111 = rolling_forecast_30_days_ahead(train_sample, test_sample, (1,1,1))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mape_010_30_days = mean_absolute_percentage_error(predictions_30_days_ahead_010, test_sample[29:])\nrmse_010_30_days = np.sqrt(mean_squared_error(test_sample[29:], predictions_30_days_ahead_010))\nmae_010_30_days = mean_absolute_error(test_sample[29:], predictions_30_days_ahead_010)\nr2_010_30_days = r2_score(test_sample[29:], predictions_30_days_ahead_010)\n\nmape_111_30_days = mean_absolute_percentage_error(predictions_30_days_ahead_111, test_sample[29:])\nrmse_111_30_days = np.sqrt(mean_squared_error(test_sample[29:], predictions_30_days_ahead_111))\nmae_111_30_days = mean_absolute_error(test_sample[29:], predictions_30_days_ahead_111)\nr2_111_30_days = r2_score(test_sample[29:], predictions_30_days_ahead_111)\n\nprint('Evaluation Metrics (30 days ahead) - ARIMA(0, 1, 0)\\n')\nprint(f\"RMSE: {rmse_010_30_days}\")\nprint(f\"MAPE: {mape_010_30_days}\")\nprint(f\"MAE: {mae_010_30_days}\")\nprint(f\"R^2: {r2_010_30_days}\")\n\nprint('\\n\\n')\nprint('Evaluation Metrics (30 days ahead) - ARIMA(1, 1, 1)\\n')\nprint(f\"RMSE: {rmse_111_30_days}\")\nprint(f\"MAPE: {mape_111_30_days}\")\nprint(f\"MAE: {mae_111_30_days}\")\nprint(f\"R^2: {r2_111_30_days}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Normalization","metadata":{}},{"cell_type":"code","source":"def normalization(df):\n    for column in df.select_dtypes(include=['number']).columns:\n        df[column] = (df[column] - df[column].min())/(df[column].max() - df[column].min()) \n    return df\n\ndata = normalization(data)\n\ndata.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Time-Series Neural Network","metadata":{}},{"cell_type":"markdown","source":"- Optimizator: Adam\n- Batch size: 32\n- Look-back period: [3, 8, 14, 30]\n- Kernel size (following LB period): [2, 3, 5, 7]\n- Number of kernels and number of neurons for time attention layer and hidden layer: [32, 64, 128]\n- Learning rate: [0.01, 0.001, 0.0001]\n- Dropout: [0.1;0.5]\n- Activation function: []","metadata":{}},{"cell_type":"markdown","source":"## Further data preparation","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(42)\nnp.random.seed(42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features = data[['Open', 'High', 'Low', 'Volume']].values\nlabels = data[['Adj Close']].values\n\nx_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\nx_tuning_train, x_val, y_tuning_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TSdataset(Dataset):\n    def __init__(self, features, labels, sequence_length, prediction_horizon):\n      self.features = features  \n      self.labels = labels  \n      self.sequence_length = sequence_length  \n      self.prediction_horizon = prediction_horizon\n\n    def __len__(self):\n        return len(self.features) - self.sequence_length - self.prediction_horizon + 1\n\n    def __getitem__(self, idx):\n        return (self.features[idx:idx + self.sequence_length], self.labels[idx + self.sequence_length + self.prediction_horizon - 1])\n\ndef create_dataloader(features_train, labels_train, features_val, labels_val, sequence_length, prediction_horizon):\n    train_dataset = TSdataset(features_train, labels_train, sequence_length, prediction_horizon)\n    val_dataset = TSdataset(features_val, labels_val, sequence_length, prediction_horizon)\n\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n    return train_loader, val_loader","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fine Tuning without Time Attention","metadata":{}},{"cell_type":"code","source":"class TNN(nn.Module):\n    def __init__(self, input_size, kernel_output_size, kernel_size, hidden_size, output_size, activation_function, dropout_rate):\n        super().__init__()\n        self.kernel_filter = nn.Conv1d(in_channels=input_size, out_channels=kernel_output_size, kernel_size=kernel_size)\n\n        self.fc1 = nn.Linear(kernel_output_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, output_size)\n\n        self.activation_function = activation_function\n        self.dropout = nn.Dropout(dropout_rate)\n\n\n    def forward(self, x):\n        x = x.permute(0, 2, 1)\n        x = self.kernel_filter(x)\n        x = torch.relu(x)\n\n        x = torch.sum(x, dim=2)\n\n        x = self.fc1(x)\n        x = self.activation_function(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return x\n\ndef train_and_validate(model, train_loader, val_loader, optimizer, epochs):\n    criterion = nn.MSELoss()\n    for epoch in range(epochs):\n        model.train()\n        for features, labels in train_loader:\n            features, labels = features.float(), labels.float().unsqueeze(1)\n            outputs = model(features)\n            loss = criterion(outputs, labels.squeeze(1))\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        model.eval()\n        val_predictions, val_targets = [], []\n        with torch.no_grad():\n            for features, labels in val_loader:\n                features, labels = features.float(), labels.float().unsqueeze(1)\n                outputs = model(features)\n                val_predictions.extend(outputs.numpy())\n                val_targets.extend(labels.numpy())\n        \n        val_rmse = np.sqrt(np.mean((np.array(val_targets) - np.array(val_predictions))**2))\n\n    return val_rmse\n\n\n\n\ndef get_kernel_size(sequence_length):\n    if sequence_length == 3:\n        return 2\n    elif sequence_length == 8:\n        return 3\n    elif sequence_length == 14:\n        return 5\n    elif sequence_length == 30:\n        return 7\n    else:\n        return 11\n\n\n\ndef grid_search(hidden_sizes, activations, dropouts, sequence_lengths, learning_rates):\n    best_val_rmse = float('inf')\n    best_params = None\n\n    activation_map = {'relu': torch.relu, 'sigmoid': torch.sigmoid, 'tanh': torch.tanh}\n    horizons = [1, 7, 30]\n\n    for hidden_size in hidden_sizes:\n        for activation_func in activations:\n            for dropout_rate in dropouts:\n                for seq_len in sequence_lengths:\n                    for lr in learning_rates:\n                        kernel_size = get_kernel_size(seq_len)\n                        print(f\"Testing with hidden_size={hidden_size}, activation={activation_func}, sequence_length={seq_len}, kernel_size={kernel_size}, lr={lr}\")\n                        \n                        model = TNN(input_size=4, kernel_output_size=32, kernel_size=kernel_size, hidden_size=hidden_size, output_size=1, activation_function=activation_map[activation_func], dropout_rate=dropout_rate)\n                        optimizer = optim.Adam(model.parameters(), lr=lr)\n\n                        avg_rmse = 0\n                        for horizon in horizons:\n                            train_loader, val_loader = create_dataloader(x_tuning_train, y_tuning_train, x_val, y_val, seq_len, horizon)\n                            horizon_rmse = train_and_validate(model, train_loader, val_loader, optimizer, epochs=100)\n                            avg_rmse += horizon_rmse\n                        avg_rmse /= len(horizons)\n                        print(f\"Average rmse across horizons: {avg_rmse:.8f}\")\n\n                        if avg_rmse < best_val_rmse:\n                            best_val_rmse = avg_rmse\n                            best_params = (hidden_size, activation_func, dropout_rate, seq_len, kernel_size, lr)\n\n\n    print(f'Best Parameters: hidden_size: {best_params[0]}, activation_func: {best_params[1]}, dropout_rate: {best_params[2]}, seq_len: {best_params[3]}, kernel_size: {best_params[4]}, lr: {best_params[5]}')\n    print(f'Best RMSE: {best_val_rmse}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hidden_sizes = [32, 64, 128]\nactivations = ['relu', 'sigmoid', 'tanh'] \ndropouts = [0.2, 0.3]\nsequence_lengths = [3, 8, 14, 30]\nlearning_rates = [0.01, 0.001, 0.0001]\n\ngrid_search(hidden_sizes, activations, dropouts, sequence_lengths, learning_rates)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fine Tuning with Time Attention","metadata":{}},{"cell_type":"code","source":"class TimeAttention(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, activation_function):\n        super().__init__()\n        self.attention_fc1 = nn.Linear(input_size, hidden_size)\n        self.attention_fc2 = nn.Linear(hidden_size, output_size)\n        self.activation_function = activation_function\n        self.attention_softmax = nn.Softmax(dim=1)\n\n\n    def forward(self, features):\n        attention_hidden = self.activation_function(self.attention_fc1(features))\n        attention_scores = self.attention_fc2(attention_hidden)\n        attention_weights = self.attention_softmax(attention_scores)\n        weighted_features = attention_weights * features\n        return weighted_features\n\nclass TNNwithAttention(nn.Module):\n    \n    def __init__(self, input_size, kernel_output_size, kernel_size, hidden_size, output_size, activation_function, dropout_rate, attention_hidden_size, attention_activation):\n        super().__init__()\n        self.kernel_filter = nn.Conv1d(in_channels=input_size, out_channels=kernel_output_size, kernel_size=kernel_size)\n        self.time_attention = TimeAttention(kernel_output_size, attention_hidden_size, kernel_output_size, activation_function)\n        self.fc1 = nn.Linear(kernel_output_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, output_size)\n        self.activation_function = activation_function\n        self.dropout = nn.Dropout(dropout_rate)\n\n\n    def forward(self, x):\n        x = x.permute(0, 2, 1)\n        x = self.kernel_filter(x)\n        x = torch.relu(x)\n        \n        x = x.permute(0, 2, 1)\n        x = self.time_attention(x)\n        x = torch.sum(x, dim=1)\n        \n        x = self.fc1(x)\n        x = self.activation_function(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return x\n        \n\ndef grid_search_with_time_attention():\n\n    best_val_rmse = float('inf')\n    best_params = None\n    \n    dropout_rate = 0.2\n    sequence_length = 3\n    learning_rate = 0.01\n    kernel_size = 2\n    horizons = [1, 7, 30]\n\n    for num_kernels in num_kernels_range:\n        for hidden_size in hidden_sizes:\n            for attention_hidden_size in attention_hidden_sizes:\n                for attention_activation_function in attention_activation_functions:\n                    print(f'Testing with num_kernels={num_kernels}, hidden_size={hidden_size}, attention_hidden_size={attention_hidden_size}, attention_activation_function={attention_activation_function}')\n\n                    model = TNNwithAttention(input_size=4, kernel_output_size=num_kernels, kernel_size=kernel_size, hidden_size=hidden_size, output_size=1, activation_function=torch.relu dropout_rate=dropout_rate, attention_hidden_size=attention_hidden_size, attention_activation=attention_activation_function)\n                    optimizer = optim.Adam(model.parameters, lr=learning_rate)\n\n                    avg_rmse = 0\n                    for horizon in horizons:\n                        train_loader,val_loader = create_dataloader(x_tuning_train, y_tuning_train, x_val, y_val, sequence_length, horizon)\n                        horizon_rmse = train_and_validate(model, train_loader, val_loader, optimizer, epochs=100)\n                        avg_rmse += horizon_rmse\n\n                    avg_rmse /= len(horizons)\n                    print(f'Average RMSE across horizons: {avg_rmse}')\n\n                    if avg_rmse < best_val_rmse:\n                        best_val_rmse = avg_rmse\n                        best_params = (num_kernels, hidden_size, attention_hidden_size, attention_activation_function)\n\n    print(f'Best parameters: num_kernels={best_params[0]}, hidden_size={best_params[1]}, attention_hidden_size={best_params[2]}, attention_activation_function={best_params[3]}')\n    print(f'Best RMSE: {best_val_rmse}')\n                        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_kernels_range = [32, 64, 128]\nhidden_sizes = [32, 64, 128]\nattention_hidden_sizes = [32, 64, 128]\nattention_activation_functions = [torch.tanh, torch.relu]\n\ngrid_search_with_time_attention(num_kernels_range, hidden_sizes, attention_hidden_sizes, attention_activation_functions)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}